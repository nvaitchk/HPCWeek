{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MountainCar-v0\n",
    "### In this notebook, you will deal with continuous state and action spaces by discretizing them and apply reinforcement learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Clean previous runs\n",
    "shutil.rmtree('runs/mountaincar_qlearning', ignore_errors=True)\n",
    "\n",
    "# Create an environment and set random seed\n",
    "env = gym.make('MountainCar-v0')\n",
    "env.seed(505)\n",
    "\n",
    "def create_uniform_grid(low, high, bins=(10, 10)):\n",
    "    x_grid_len = (high[0] - low[0])/(bins[0]*1.0)\n",
    "    y_grid_len = (high[1] - low[1])/(bins[1]*1.0)\n",
    "    x_grid = [low[0]+i*x_grid_len for i in range(1, bins[0])]\n",
    "    y_grid = [low[1]+i*y_grid_len for i in range(1, bins[1])]\n",
    "    return [np.array(x_grid), np.array(y_grid)]\n",
    "\n",
    "def discretize(sample, grid):\n",
    "    x = np.digitize(np.array([sample[0]]), grid[0], right=False)\n",
    "    y = np.digitize(np.array([sample[1]]), grid[1], right=False)\n",
    "    return [x[0], y[0]]\n",
    "\n",
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent that can act on a continuous state space by discretizing it.\"\"\"\n",
    "\n",
    "    def __init__(self, env, state_grid, alpha=0.02, gamma=0.99,\n",
    "                 epsilon=1.0, epsilon_decay_rate=0.9995, min_epsilon=.01, seed=505):\n",
    "        self.env = env\n",
    "        self.state_grid = state_grid\n",
    "        self.state_size = tuple(len(splits) + 1 for splits in self.state_grid)\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.seed = np.random.seed(seed)\n",
    "        print(\"Environment:\", self.env)\n",
    "        print(\"State space size:\", self.state_size)\n",
    "        print(\"Action space size:\", self.action_size)\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = self.initial_epsilon = epsilon\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.q_table = np.zeros(shape=(self.state_size + (self.action_size,)))\n",
    "\n",
    "    def preprocess_state(self, state):\n",
    "        return tuple(discretize(state, self.state_grid))\n",
    "\n",
    "    def reset_episode(self, state):\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "        self.last_state = self.preprocess_state(state)\n",
    "        self.last_action = np.argmax(self.q_table[self.last_state])\n",
    "        return self.last_action\n",
    "    \n",
    "    def reset_exploration(self, epsilon=None):\n",
    "        self.epsilon = epsilon if epsilon is not None else self.initial_epsilon\n",
    "\n",
    "    def act(self, state, reward=None, done=None, mode='train'):\n",
    "        state = self.preprocess_state(state)\n",
    "        if mode == 'test':\n",
    "            action = np.argmax(self.q_table[state])\n",
    "        else:\n",
    "            self.q_table[self.last_state + (self.last_action,)] += self.alpha * (\n",
    "                reward + self.gamma * max(self.q_table[state]) - self.q_table[self.last_state + (self.last_action,)]\n",
    "            )\n",
    "            td_error = reward + self.gamma * max(self.q_table[state]) - self.q_table[self.last_state+(self.last_action,)]\n",
    "            do_exploration = np.random.uniform(0, 1) < self.epsilon\n",
    "            if do_exploration:\n",
    "                action = np.random.randint(0, self.action_size)\n",
    "            else:\n",
    "                action = np.argmax(self.q_table[state])\n",
    "        self.last_state = state\n",
    "        self.last_action = action\n",
    "        return action\n",
    "    \n",
    "def run(agent, env, num_episodes=20000, mode='train', writer=None):\n",
    "    scores = []\n",
    "    max_avg_score = -np.inf\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        action = agent.reset_episode(state)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        episode_losses = []\n",
    "        while not done:\n",
    "            prev_state = agent.last_state\n",
    "            prev_action = agent.last_action\n",
    "            state, reward, done, info = env.step(action)\n",
    "            total_reward += reward\n",
    "            action = agent.act(state, reward, done, mode)\n",
    "            td_target = reward + agent.gamma * max(agent.q_table[agent.preprocess_state(state)])\n",
    "            td_error = td_target - agent.q_table[prev_state + (prev_action,)]\n",
    "            episode_losses.append(abs(td_error))\n",
    "        scores.append(total_reward)\n",
    "        if writer is not None:\n",
    "            writer.add_scalar(\"Reward/Episode\", total_reward, i_episode)\n",
    "            if episode_losses:\n",
    "                avg_loss = np.mean(episode_losses)\n",
    "                writer.add_scalar(\"Loss/Episode\", avg_loss, i_episode)\n",
    "        if mode == 'train':\n",
    "            if len(scores) > 100:\n",
    "                avg_score = np.mean(scores[-100:])\n",
    "                if avg_score > max_avg_score:\n",
    "                    max_avg_score = avg_score\n",
    "            if i_episode % 100 == 0:\n",
    "                print(\"\\rEpisode {}/{} | Max Average Score: {}\".format(i_episode, num_episodes, max_avg_score), end=\"\")\n",
    "                sys.stdout.flush()\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can modify the parameters, re-train the agent and observe changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY the hyperparameters here\n",
    "alpha=0.02\n",
    "gamma=0.99\n",
    "epsilon=1.0\n",
    "epsilon_decay_rate=0.9995\n",
    "\n",
    "# Create discretization grid\n",
    "state_grid_new = create_uniform_grid(env.observation_space.low, env.observation_space.high, bins=(20, 20))\n",
    "\n",
    "# Initialize agent\n",
    "q_agent_new = QLearningAgent(env, state_grid_new, alpha, gamma, epsilon, epsilon_decay_rate)\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/mountaincar_qlearning\")\n",
    "q_agent_new.scores = []\n",
    "\n",
    "# Train agent if Q-table does not exist\n",
    "q_table_filename = f'trained_q_table_a{alpha}_g{gamma}_e{epsilon}_d{epsilon_decay_rate}.npy'\n",
    "if not os.path.exists(q_table_filename):\n",
    "    print(\"Training agent...\")\n",
    "    q_agent_new.scores += run(q_agent_new, env, num_episodes=15000, writer=writer)\n",
    "    np.save(q_table_filename, q_agent_new.q_table)\n",
    "    print(\"\\nTraining complete. Q-table saved.\")\n",
    "else:\n",
    "    print(\"Loading existing Q-table...\")\n",
    "    q_agent_new.q_table = np.load(q_table_filename)\n",
    "\n",
    "writer.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can watch the replay of your trained agent. Enter 'r' to replay, or 'q' to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watch Replay\n",
    "while True:\n",
    "    user_input = input(\"\\nPress 'r' to watch the agent, or 'q' to quit: \")\n",
    "    if user_input.lower() == 'q':\n",
    "        print(\"Exiting.\")\n",
    "        break\n",
    "    elif user_input.lower() == 'r':\n",
    "        view_env = gym.make('MountainCar-v0')\n",
    "        state = view_env.reset()\n",
    "        score = 0\n",
    "        for t in range(200):\n",
    "            action = q_agent_new.act(state, mode='test')\n",
    "            view_env.render()\n",
    "            state, reward, done, _ = view_env.step(action)\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "        print(f'Final score: {score}')\n",
    "        view_env.close()\n",
    "    else:\n",
    "        print(\"Invalid input. Please use 'r' or 'q'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can start a tensorboard to see changes of the agent training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#restart the kernel everytime you run the program with different parmeters to see changes in tensorboard\n",
    "!tensorboard --logdir=runs/mountaincar_qlearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open a new tab in the browser and type:\n",
    "http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
